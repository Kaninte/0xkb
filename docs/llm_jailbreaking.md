# LLM Jailbreaking
LLM jailbreaking refers to techniques used to bypass the ethical, safety, or security restrictions imposed on large language models. This involves tricking the model into generating restricted content, circumventing safeguards through prompt engineering, token manipulation, context exploitation, or indirect attacks.

